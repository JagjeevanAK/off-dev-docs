---
title: "Mitigate Crawlers on Prod"
description: "Learn how to mitigate excessive crawler traffic on production servers. This guide covers identifying problematic IPs, warning users, blacklisting IPs using fail2ban and iptables, and suggests implementing NGINX traffic rate limiting for better performance."
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Card, Cards } from 'fumadocs-ui/components/card';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';

# How to Mitigate Crawlers with Excessive Traffic

<Callout type="warn">
Some programs may hammer Open Food Facts servers, causing serious latency and 504 errors for all users. This guide helps identify and mitigate such issues.
</Callout>

## Background

<Card title="Common Scenario" description="Understanding the typical crawler problem">

Most excessive crawler traffic comes from users who are unaware that bulk data should be downloaded from [https://world.openfoodfacts.org/data](https://world.openfoodfacts.org/data) instead of web scraping.

</Card>

## Mitigation Process

<Steps>
  <Step title="Identify Problematic IPs">
    Use access logs to find IPs making excessive requests
  </Step>
  
  <Step title="Analyze Traffic Patterns">
    Examine specific IP activity to understand the behavior
  </Step>
  
  <Step title="Attempt Contact">
    Try to reach out to users when possible
  </Step>
  
  <Step title="Implement Blocking">
    Use appropriate blocking mechanisms based on server configuration
  </Step>
</Steps>

## Finding Disturbing IPs

<Card title="Log Analysis Tools" description="Using concatenate_by_ip.pl script for traffic analysis">

### Access Log Analysis

Navigate to your site's access log directory and use the traffic analysis script:

```bash title="Find Top Request IPs"
tail -n 100000 access_log | ./concatenate_by_ip.pl | tail -n 10
```

This command shows IP addresses ranked by number of requests.

### Individual IP Investigation

```bash title="Analyze Specific IP Activity"
tail -n 100000 access_log | grep <ip_address>
```

Use this to examine the activity pattern of a specific IP address.

<Callout type="info">
Search and facets requests are typically the most resource-consuming operations. Product read requests are usually just file reads.
</Callout>

</Card>

## Contact Attempts

<Card title="User Outreach" description="Try to educate users before blocking">

If the User-Agent string provides contact information or identifies the organization:

<Steps>
  <Step title="Identify Contact">
    Look for email addresses, organization names, or project identifiers in User-Agent strings
  </Step>
  
  <Step title="Educational Approach">
    Inform them about the proper data download endpoints at `/data`
  </Step>
  
  <Step title="Provide Alternatives">
    Direct them to appropriate API documentation and bulk data sources
  </Step>
</Steps>

</Card>

## Blocking Methods

<Tabs items={["OFF2 Reverse Proxy", "OFF1 Direct", "Future Improvements"]}>
  <Tab value="OFF2 Reverse Proxy">
    ### Using Fail2ban on OFF2
    
    For the reverse proxy server, use the fail2ban nginx-botsearch jail:
    
    ```bash title="Ban IP on OFF2"
    fail2ban-client set nginx-botsearch banip <IP>
    ```
    
    <Callout type="tip">
    See the [How to use fail2ban to ban bots](./how-to-fail2ban-ban-bots.md) guide for more details.
    </Callout>
  </Tab>
  
  <Tab value="OFF1 Direct">
    ### Using iptables on OFF1
    
    For direct blocking on OFF1, use iptables rules:
    
    ```bash title="Block IP with iptables"
    iptables -A INPUT -s <ip_address> -j DROP
    ```
    
    <Callout type="warn">
    This method immediately drops all traffic from the specified IP address.
    </Callout>
  </Tab>
  
  <Tab value="Future Improvements">
    ### Rate Limiting Implementation
    
    <Card title="NGINX Rate Limiting" description="A more sophisticated approach to traffic management">
    
    Instead of outright blocking, implementing NGINX traffic rate limitation would provide:
    
    - **Graceful degradation** instead of complete blocking
    - **Fair usage** enforcement for all users
    - **Automatic throttling** without manual intervention
    - **Better user experience** for legitimate heavy users
    
    </Card>
  </Tab>
</Tabs>

## Traffic Analysis Tips

<Cards>
  <Card title="High-Impact Requests" description="Search and facet operations consume most resources" />
  <Card title="Low-Impact Requests" description="Product page views are typically just file reads" />
  <Card title="Pattern Recognition" description="Look for repeated patterns in request URLs" />
  <Card title="User-Agent Analysis" description="Check for bot identification or contact information" />
</Cards>

## Prevention Strategies

<Card title="Long-term Solutions" description="Preventing crawler issues before they impact production">

### Proactive Measures

<Steps>
  <Step title="Rate Limiting">
    Implement NGINX rate limiting to automatically throttle excessive requests
  </Step>
  
  <Step title="Documentation">
    Improve visibility of bulk data download options
  </Step>
  
  <Step title="API Guidelines">
    Provide clear API usage guidelines and rate limits
  </Step>
  
  <Step title="Monitoring">
    Set up alerts for unusual traffic patterns
  </Step>
</Steps>

</Card>

## Related Documentation

<Cards>
  <Card 
    title="Fail2ban Bot Blocking" 
    description="Detailed guide on using fail2ban to manage bot traffic"
    href="./how-to-fail2ban-ban-bots.md"
  />
  <Card 
    title="Data Downloads" 
    description="Official bulk data download location"
    href="https://world.openfoodfacts.org/data"
  />
</Cards>
