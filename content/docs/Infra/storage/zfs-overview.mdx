---
title: "ZFS Overview"
description: "This document provides a comprehensive overview of ZFS, detailing its reliability, incredible capabilities, and essential features like data synchronization through snapshots. It also covers learning resources, useful commands, fine-tuning tips, and integration with Proxmox and Docker."
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Card, Cards } from 'fumadocs-ui/components/card';
import { Steps, Step } from 'fumadocs-ui/components/steps';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';

# ZFS Overview

## Why ZFS?

<Callout type="info">
We use a lot ZFS for our data for its reliability and incredible capabilities. The most important feature is data synchronization through snapshots. Clone also enables to easily have same data as production for tests.
</Callout>

## Learning Resources

<Cards>
<Card title="ZFS Introduction" href="reports/2023-02-24-zfs-introduction.md">
The onboarding made by Christian - great starting point
</Card>
<Card title="OpenZFS Official Documentation" href="https://openzfs.github.io/openzfs-docs/">
Very complete documentation (though a bit hard to navigate)
</Card>
<Card title="Proxmox ZFS Documentation" href="https://pve.proxmox.com/pve-docs/pve-admin-guide.html#chapter_zfs">
Proxmox-specific ZFS information and Wiki
</Card>
<Card title="Ubuntu ZFS Tutorial" href="https://ubuntu.com/tutorials/using-zfs-snapshots-clones#1-overview">
Tutorial about ZFS snapshots and clone
</Card>
<Card title="ZFS Cheat Sheet" href="https://blog.mikesulsenti.com/zfs-cheat-sheet-and-guide/">
A good cheat-sheet for quick reference
</Card>
</Cards>

### Key Documentation Sections

<Tabs>
<Tab value="misc" label="Misc Man Pages">
[Misc man page section](https://openzfs.github.io/openzfs-docs/man/master/7/index.html) contains a lot of useful information on properties and features
</Tab>

<Tab value="concepts" label="Basic Concepts">
[Basic Concepts](https://openzfs.github.io/openzfs-docs/Basic%20Concepts/index.html) is a good place to return to from time to time to grab concepts better
</Tab>

<Tab value="admin" label="System Administration">
[Man pages on system administration commands](https://openzfs.github.io/openzfs-docs/man/master/8/index.html) are really useful
</Tab>

<Tab value="performance" label="Performance and Tuning">
[Performance and Tuning](https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/index.html) is also valuable to dig in some options
</Tab>
</Tabs>

## Useful Commands

<Cards>
<Card title="Pool Status">
`zpool status` - See eventual errors
</Card>
<Card title="Device List">
`zpool list -v` - See all devices
</Card>
<Card title="Dataset List">
`zfs list -r` - Get all datasets and their mountpoints
</Card>
<Card title="I/O Statistics">
`zpool iostat` - See stats about read / write
</Card>
<Card title="Pool History">
`zpool history` - List all operations done on a pool
</Card>
<Card title="Space Usage">
`zpool list -o name,size,usedbysnapshots,allocated` - See space allocated
</Card>
</Cards>

### Important Command Notes

<Callout type="warning">
**ALLOC Quirk**: There is a quirk with ALLOC which is different for mirror pools and raidz pools. On the first it's allocated space to datasets, on the second it's used space.
</Callout>

<Callout type="info">
**Free Space**: `df` on a dataset does not really work because free space is shared between the datasets. You can still see datasets usage by using:

```bash title="Dataset Usage"
zfs list -o name,used,usedbydataset,usedbysnapshots,available -r <pool_name>
```
</Callout>

### Advanced Commands

<Tabs>
<Tab value="iostat" label="I/O Statistics">
```bash title="Detailed I/O Stats"
zpool iostat -vl 5  # Particularly useful
zpool iostat -w     # Helps understand time taken by data to be read/written
```
</Tab>

<Tab value="zdb" label="ZFS Debugger">
```bash title="ZFS Database Info"
zdb -s <poolname>  # Pool statistics
```

[`zdb`](https://openzfs.github.io/openzfs-docs/man/master/8/zdb.8.html) is also worth knowing for debugging
</Tab>
</Tabs>

## Fine-Tuning

<Cards>
<Card title="Compression">
We use compression on a lot of our datasets (`compression=on`)
</Card>
<Card title="Cache Optimization">
You can set `primarycache=metadata` for volumes with large number of files that are not critical to read fast (e.g., Images on off container, or backups from other servers)
</Card>
<Card title="Performance Monitoring">
In [Munin](./munin.md) `ZFS ARC - Hit ratio` the `ZFS Arc Efficiency` gives you an idea of how well ZFS caching is performing
</Card>
</Cards>

## Integration with Other Systems

<Tabs>
<Tab value="proxmox" label="Proxmox">
<Callout type="info">
Proxmox uses ZFS to replicate containers and VMs between servers. It also uses it to backup data.

**Note**: We progressively abandon Proxmox ZFS replication in favor of using sanoid/syncoid.
</Callout>
</Tab>

<Tab value="sanoid" label="Sanoid">
<Card title="Sanoid Integration" href="./sanoid.md">
We use sanoid / syncoid to sync ZFS datasets between servers (also to back them up)
</Card>
</Tab>
</Tabs>

## How-To Guides

### How to Create a ZPool

<Steps>
<Step>
**Check Disk Availability**
Ensure disks are not mounted and available (e.g., using `lsblk`). You can eventually split them into different partitions if needed.
</Step>

<Step>
**Create the Pool**
```bash title="Create ZPool"
zpool create <pool_name> <pool-type> <device1> <device2> ...
```

- For pool name use something like zfs-something (zfs-hdd, zfs-nvme, etc.)
- For pool-type, use the one that's needed: none (no mention), mirror, raidz
</Step>

<Step>
**Optional: Add Cache/Log Devices**
You may add a [cache](https://openzfs.github.io/openzfs-docs/man/master/7/zpoolconcepts.7.html#Cache_Devices) and/or a [log device](https://openzfs.github.io/openzfs-docs/man/master/7/zpoolconcepts.7.html#log).
</Step>
</Steps>

### How to NFS Mount a ZFS Dataset

<Callout type="info">
ZFS directly integrates to NFS server.
</Callout>

<Steps>
<Step>
**Set ShareNFS Property**
To have a dataset shared in NFS, you have to set sharenfs property to allowed addresses and other options.

Examples:
```bash title="NFS Sharing Examples"
zfs set sharenfs="rw=@10.0.0.0/28,no_root_squash" <pool_name>/<dataset_name>
zfs set sharenfs="rw=@10.0.0.1/32,rw=@10.1.0.200/32,no_root_squash" <pool_name>/<dataset_name>
```
</Step>
</Steps>

<Callout type="danger">
**Very important**: Always filter on an internal sub network, otherwise your Dataset is exposed to the internet!!!

Beware that all descendants inherit the property and will also be shared.
</Callout>

### How to Replace a Disk in ZPool

<Steps>
<Step>
**List Devices**
Get a list of devices using `zpool status <pool_name>`
</Step>

<Step>
**Put Disk Offline**
```bash title="Offline Disk"
zpool <pool_name> offline <device_name>  # e.g., zpool rpool offline sdf
```
</Step>

<Step>
**Replace Physical Disk**
Replace the disk physically
</Step>

<Step>
**Replace in ZPool**
```bash title="Replace in ZPool"
zpool <pool_name> replace /dev/<device_name>  # e.g., zpool rpool replace /dev/sdf
```
</Step>

<Step>
**Verify Resilvering**
Verify disk is back and being resilvered:

```bash title="Check Status"
zpool status <pool_name>
```

Expected output showing resilvering:
```
 state: DEGRADED
status: One or more devices is currently being resilvered...
  scan: resilver in progress since ...
...
        replacing-5  DEGRADED     0     0     0
        old        OFFLINE      0     0     0
        sdf        ONLINE       0     0     0  (resilvering)
```
</Step>

<Step>
**Optional: Run Scrub**
After resilver finishes, you can eventually run a scrub:

```bash title="Run Scrub"
zpool scrub <pool_name>
```
</Step>
</Steps>

### How to Sync ZFS Datasets

<Callout type="info">
To Sync ZFS you just take snapshots on the source at specific intervals (we use cron jobs). You then use [zfs-send](https://openzfs.github.io/openzfs-docs/man/8/zfs-send.8.html) and [zfs-recv](https://openzfs.github.io/openzfs-docs/man/8/zfs-recv.8.html) through ssh to sync the distant server (send snapshots).
</Callout>

<Tabs>
<Tab value="automatic" label="Automatic Sync">
<Card title="Automated Syncing" href="./sanoid.md">
We normally do it using sanoid and syncoid
</Card>

Proxmox might also do it as part of corosync to replicate containers across cluster.
</Tab>

<Tab value="manual" label="Manual Sync">
```bash title="Manual ZFS Sync"
zfs send <previous-snap> <dataset_name>@<last-snap> \
  | ssh <hostname> zfs recv <target_dataset_name> -F
```

<Card title="STO Products Sync" href="https://github.com/openfoodfacts/openfoodfacts-infrastructure/blob/develop/scripts/off1/sto-products-sync.sh">
ZFS sync of sto files from off1 to off2
</Card>

<Callout type="warning">
You also have to clean snapshots from time to time to avoid retaining too much useless data.

On ovh3: [snapshot-purge.sh](https://github.com/openfoodfacts/openfoodfacts-infrastructure/blob/develop/scripts/ovh3/snapshot-purge.sh)
</Callout>
</Tab>
</Tabs>

### How to Docker Mount a ZFS Dataset

<Cards>
<Card title="Local Mount">
If ZFS dataset is on same machine we can use bind mounts to mount a folder in a ZFS partition
</Card>
<Card title="Remote Mount">
For distant machines, ZFS datasets can be exposed as NFS partition. Docker has an integrated driver to mount distant NFS as volumes
</Card>
</Cards>

### How to Mount Datasets in a Proxmox Container

To mount dataset in a proxmox container you have two options:

<Tabs>
<Tab value="shared-disk" label="Shared Disk">
<Callout type="info">
Not really experimented yet, but it could have the advantage to enable replication.
</Callout>

<Steps>
<Step>
On your VM/CT, in resource, add a disk.
</Step>
<Step>
Add the mountpoint to your disk and declare it shared.
</Step>
<Step>
In another VM/CT, add the same disk.
</Step>
</Steps>
</Tab>

<Tab value="bind-volumes" label="Bind Volumes">
See: https://pve.proxmox.com/wiki/Linux_Container#_bind_mount_points and https://pve.proxmox.com/wiki/Unprivileged_LXC_containers

<Steps>
<Step>
**Edit Container Configuration**
Edit `/etc/pve/lxc/<container_id>.conf` and add volumes with mount point.

Example:
```bash title="Container Configuration"
# volumes
mp0: /zfs-hdd/opff,mp=/mnt/opff
mp1: /zfs-hdd/opff/products/,mp=/mnt/opff/products
mp2: /zfs-hdd/off/users/,mp=/mnt/opff/users
mp3: /zfs-hdd/opff/images,mp=/mnt/opff/images
mp4: /zfs-hdd/opff/html_data,mp=/mnt/opff/html_data
mp5: /zfs-hdd/opff/cache,mp=/mnt/opff/cache
```

<Callout type="warning">
**Important**: If you have nested mount points, the order is very important. First the outermost, then the inner ones.
</Callout>
</Step>

<Step>
**Reboot Container**
To take changes in account, you have to reboot:

```bash title="Reboot Container"
pct reboot <container_id>
```
</Step>
</Steps>
</Tab>
</Tabs>

#### Getting UIDs and GIDs Right

<Callout type="info">
LXC maps uids inside the container to specific ids outside, most of the time by adding a large value. It's a way to ensure security.
</Callout>

<Steps>
<Step>
**Edit Sub UID/GID**
If you want to have file belonging to say uid 1000 in the zfs mount, you will have to tweak it:

We edit `/etc/subuid` and `/etc/subgid` to add `root:1000:10`. This allows container started by root to map ids 1000 to their same ids on system.
</Step>

<Step>
**Configure Container ID Mapping**
Edit `/etc/pve/lxc/<machine_id>.conf` to add sub_id exceptions:

```bash title="UID/GID Mapping"
# uid map: from uid 0 map 999 uids (in the ct) to the range starting 100000 (on the host)
# so 0..999 (ct) → 100000..100999 (host)
lxc.idmap = u 0 100000 999
lxc.idmap = g 0 100000 999
# we map 10 uid starting from uid 1000 onto 1000, so 1000..1010 → 1000..1010
lxc.idmap = u 1000 1000 10
lxc.idmap = g 1000 1000 10
# we map the rest of 65535 from 1010 upto 101010, so 1010..65535 → 101010..165535
lxc.idmap = u 1011 101011 64525
lxc.idmap = g 1011 101011 64525
```
</Step>
</Steps>